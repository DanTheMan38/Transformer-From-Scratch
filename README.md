# Transformer-From-Scratch

## Description
A custom-built Transformer-based language model developed from scratch for text generation and NLP tasks. This project implements the foundational elements of a Transformer, aiming to understand and replicate the architecture that powers models like GPT-2.

## Project Update
This project has been an incredible learning journey, but I have decided to pause further development to focus on other ventures. I hope the code and insights provided here will still serve as a valuable resource for anyone exploring Transformers and NLP tasks. Thank you for your interest and support!

## Features
- Build a Transformer model from scratch.
- Train the model on custom datasets for various NLP tasks.
- Explore and experiment with Transformer components, including self-attention and positional encoding.

## Requirements
Install the required packages with:
```
pip install -r requirements.txt
```

### `requirements.txt` includes:
```
torch
transformers
numpy
pandas
tqdm
scikit-learn
```

## Usage
1. Clone the repository:
   ```
   git clone https://github.com/DanTheMan38/Transformer-From-Scratch.git
   ```

2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

3. Run the model training script:
   ```
   python train_transformer.py
   ```

## License
This project is licensed under the MIT License.